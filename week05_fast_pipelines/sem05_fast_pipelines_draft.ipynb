{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f720956",
   "metadata": {},
   "source": [
    "# Week 5: Fast pipelines, profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b7b98",
   "metadata": {},
   "source": [
    "## Automatic mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fb3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd0d9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdac7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing utilities\n",
    "start_time = None\n",
    "def start_timer():\n",
    "    global start_time\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time()\n",
    "\n",
    "def end_timer_and_print(local_msg):\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time()\n",
    "    print(\"\\n\" + local_msg)\n",
    "    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n",
    "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aac0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(in_size, out_size, num_layers):\n",
    "    layers = []\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(torch.nn.Linear(in_size, in_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(in_size, out_size))\n",
    "    return torch.nn.Sequential(*tuple(layers)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb66edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512 # Try, for example, 128, 256, 513.\n",
    "in_size = 8192\n",
    "out_size = 8192\n",
    "num_layers = 3\n",
    "num_batches = 50\n",
    "epochs = 3\n",
    "\n",
    "# Creates data in default precision.\n",
    "# The same data is used for both default and mixed precision trials below.\n",
    "# You don't need to manually change inputs' dtype when enabling mixed precision.\n",
    "data = [torch.randn(batch_size, in_size, device=\"cuda\") for _ in range(num_batches)]\n",
    "targets = [torch.randn(batch_size, out_size, device=\"cuda\") for _ in range(num_batches)]\n",
    "\n",
    "loss_fn = torch.nn.MSELoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75cde7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759cc61590724280ae28a8d212b91fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default precision:\n",
      "Total execution time = 42.162 sec\n",
      "Max memory used by tensors = 6375868928 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "start_timer()\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        output = net(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "end_timer_and_print(\"Default precision:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "566120f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7538dc126c274d6ca9d3237a412dc71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Half precision:\n",
      "Total execution time = 61.051 sec\n",
      "Max memory used by tensors = 6979913728 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "start_timer()\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        net.half()\n",
    "        output = net(input.half())\n",
    "        loss = loss_fn(output, target.half())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "end_timer_and_print(\"Half precision:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25f89d",
   "metadata": {},
   "source": [
    "TBD: half precision + loss scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c1363c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AMP:\n",
      "Total execution time = 126.234 sec\n",
      "Max memory used by tensors = 6308579328 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "start_timer()\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        # Runs the forward pass under autocast.\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = net(input)\n",
    "            # output is float16 because linear layers autocast to float16.\n",
    "            assert output.dtype is torch.float16\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "            # loss is float32 because mse_loss layers autocast to float32.\n",
    "            assert loss.dtype is torch.float32\n",
    "\n",
    "        # Exits autocast before backward().\n",
    "        # Backward passes under autocast are not recommended.\n",
    "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "end_timer_and_print(\"AMP:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feda3bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AMP with loss scaling:\n",
      "Total execution time = 130.169 sec\n",
      "Max memory used by tensors = 6308579328 bytes\n"
     ]
    }
   ],
   "source": [
    "# Constructs scaler once, at the beginning of the convergence run, using default args.\n",
    "# If your network fails to converge with default GradScaler args, please file an issue.\n",
    "# The same GradScaler instance should be used for the entire convergence run.\n",
    "# If you perform multiple convergence runs in the same script, each run should use\n",
    "# a dedicated fresh GradScaler instance.  GradScaler instances are lightweight.\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "start_timer()\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = net(input)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(opt)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "\n",
    "end_timer_and_print(\"AMP with loss scaling:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9613dd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed precision:\n",
      "Total execution time = 130.090 sec\n",
      "Max memory used by tensors = 6308579328 bytes\n"
     ]
    }
   ],
   "source": [
    "use_amp = True\n",
    "\n",
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "start_timer()\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            output = net(input)\n",
    "            loss = loss_fn(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "end_timer_and_print(\"Mixed precision:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611dbee",
   "metadata": {},
   "source": [
    "### Gradients modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac197fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = net(input)\n",
    "            loss = loss_fn(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(opt)\n",
    "\n",
    "        # Since the gradients of optimizer's assigned params are now unscaled, clips as usual.\n",
    "        # You may use the same value for max_norm here as you would without gradient scaling.\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.1)\n",
    "\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34235eea",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "Standard batching approach is just to stack tensors aquired with `__getitem__`.\n",
    "\n",
    "- Sequences with varying length\n",
    "- Different labels dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b4e4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca200720",
   "metadata": {},
   "source": [
    "brain: pad everything to a fixed max_length\n",
    "\n",
    "big brain: pad only in the collate_fn\n",
    "\n",
    "ultra duper big brain: presort data to sample sequences smartly, preserving +- the same length in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db0ada49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch:\n",
      "tensor([[ 1,  3, 71, 10, 79,  0,  0,  0,  0,  0,  0],\n",
      "        [78,  3,  5, 52, 66, 20, 38,  3,  6,  7, 17]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[39,  3,  6, 60, 25, 32,  0,  0],\n",
      "        [67, 26,  2, 40,  3,  6, 17, 84]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[82,  1, 63,  5, 62, 85, 43,  0,  0],\n",
      "        [ 2, 58, 44, 54, 21,  7, 75,  2, 19]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[ 4, 34, 76, 59, 30,  0,  0,  0,  0],\n",
      "        [ 2, 81,  1,  8, 56,  5, 12, 27, 23]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[22,  9,  4, 15,  1, 14,  3,  6,  7, 18],\n",
      "        [ 1, 14,  3,  6,  7, 18, 53, 16, 27, 77]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[57, 73,  9, 65, 20,  2, 37, 74, 69],\n",
      "        [ 8,  2, 17,  0,  0,  0,  0,  0,  0]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[11, 55, 13, 80, 26,  0,  0,  0],\n",
      "        [12,  1, 49, 33, 29,  4, 68, 72]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[12,  4, 11, 23, 70, 25,  4, 86],\n",
      "        [ 2, 51,  5, 46,  5, 61,  1,  8]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[ 1,  8, 48, 28,  0,  0,  0,  0,  0,  0],\n",
      "        [22,  9,  4, 15,  1, 14,  3,  6,  7, 18]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[ 2, 19, 10, 16, 21, 50, 10, 47,  0,  0,  0],\n",
      "        [31, 13, 64, 24, 13, 11, 83,  2, 19, 10, 16]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[ 5, 45,  9, 42, 11,  0,  0,  0,  0,  0],\n",
      "        [12,  1, 35, 41,  5,  4, 15, 24,  4, 36]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: substitute with DataSet with gititem\n",
    "lines = [\n",
    "    \"One thing I don't know why\",\n",
    "    \"It doesn't even matter how hard you try\",\n",
    "    \"Keep that in mind, I designed this rhyme\",\n",
    "    \"To explain in due time\",\n",
    "    \"All I know\",\n",
    "    \"Time is a valuable thing\",\n",
    "    \"Watch it fly by as the pendulum swings\",\n",
    "    \"Watch it count down to the end of the day\",\n",
    "    \"The clock ticks life away\",\n",
    "    \"It's so unreal\",\n",
    "    \"Didn't look out below\",\n",
    "    \"Watch the time go right out the window\",\n",
    "    \"Tryin' to hold on, did-didn't even know\",\n",
    "    \"I wasted it all just to watch you go\",\n",
    "    \"I kept everything inside and even though I tried\",\n",
    "    \"It all fell apart\",\n",
    "    \"What it meant to me will eventually\",\n",
    "    \"Be a memory of a time when I tried so hard\",\n",
    "    \"I tried so hard and got so far\",\n",
    "    \"But in the end it doesn't even matter\",\n",
    "    \"I had to fall to lose it all\",\n",
    "    \"But in the end it doesn't even matter\"\n",
    "]\n",
    "labels = torch.randint(2, (len(lines), ))\n",
    "dataset = list(zip(lines, labels))\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, label in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(iter(dataset)), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "\n",
    "def collate_batch(batch: list[tuple[str, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    text_list, label_list = [], []\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "  \n",
    "    text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "  \n",
    "    return text_list, label_list\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=2, \n",
    "    collate_fn=collate_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for x, _ in dataloader:\n",
    "    print(f\"Current batch:\\n{x}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23bdfc9",
   "metadata": {},
   "source": [
    "Also check out `transformers.DataCollatorWithPadding` at https://huggingface.co/docs/transformers/main_classes/data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615de33",
   "metadata": {},
   "source": [
    "## Data preprocessing with DALI\n",
    "https://docs.nvidia.com/deeplearning/dali/user-guide/docs/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd7be0",
   "metadata": {},
   "source": [
    "Augmentations: https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/image_processing/augmentation_gallery.html\n",
    "\n",
    "TODO: select simple example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46eb676",
   "metadata": {},
   "source": [
    "For images check out source for ResNet50 on ImageNet: https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/use_cases/pytorch/resnet50/pytorch-resnet50.ht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836dae5",
   "metadata": {},
   "source": [
    "## Albumentations\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908b398",
   "metadata": {},
   "source": [
    "## Streaming datasets\n",
    "\n",
    "What if we do not want to wait until dataset is downloaded? Terrabytes of data.\n",
    "\n",
    "What if we do not have enough disk space?\n",
    "\n",
    "https://huggingface.co/docs/datasets/dataset_streaming.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d90a837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d57cd92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'text': 'Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help.\\nEstablished in honor of John & Lindy’s son, Christopher Blanchard, this particular program is very dear to the Blanchard family. Dana Blanchard, or Mama Dana as she is more commonly referred to at Mtendere, lived on site during the initial development, and she returns each summer to spend the season with her Malawian family. The heart of the program is to be His hands and feet by caring for the children at Mtendere, and meeting their spiritual, physical, academic, and emotional needs.\\nMtendere Village is home to 134 children, living in 16 homes with a housemother and several brothers and sisters. This family environment is one that many of the children have never previously experienced. 100X has also built a library and multipurpose building on the property—giving the children a place to gather together, learn and build community.\\nThe children attend a local primary school and receive afterschool tutoring at home in the afternoon. All of the school fees, uniforms, and supplies are paid for through our sponsorship program. Once children progress to secondary school (9th grade), they attend boarding school and return home during school breaks. Through education, these children will have the resources necessary to provide a better future for themselves and for their families—one where they are never faced with the decision of whether or not to abandon their own children.\\nMaternal Health. Right now, 1 in 18 women in Malawi will die during their lifetime due to complications and the lack of medical care during pregnancy and delivery. For every 100,000 births that occur in Malawi this year, more than 800 mothers will die during delivery or shortly after because of complications. The correlation between high rates of maternal mortality and the number of orphaned children in a country is strong, and Malawi estimates that there are already more than one million orphans.\\n100X Development is working with several key government, nonprofit, and academic partners to develop the Live to Love program. Live to Love will provide education, nutrition and HIV screening to pregnant mothers, and then strengthen the capacity of midwives and community healthcare workers to respond to the needs of mothers in areas where obstetric care is systematically lacking.\\nCommunity Health. Through local collaboration with partners such as Blessings Hospital, Lumbadzi Clinic and the Kamuzu College of Nursing, we provide health assessments and treatment to women and children in the rural villages of Malawi. For many, this is their only access to healthcare.\\nIn some respects, it was the partnership with Blessings Hospital that catalyzed our involvement in Malawi. We are committed to expanding resources available through Blessings, and are currently working with them to provide an outpatient clinic, a free “Under 5” clinic, a weekly pediatric physical therapy session, and a free ART clinic for patients who are HIV+.\\nNursing Education. In a country with only 16 obstetricians serving a population of over 16 million, it is evident that nurses play a critical role in the healthcare system. With high rates of maternal death, and the 10th highest infant mortality rate in the world, it is essential that the hospitals and clinics in Malawi be staffed with fully trained medical personnel. Unfortunately, while there are more nurses than doctors, the human resource shortage also extends to nurses, and services in rural areas are limited.\\nThrough partnership with Kamuzu College of Nursing (Malawi) and Auburn University’s School of Nursing (US), we are working to train and equip nursing students with the resources they need to care for the well-being of every patient.\\nIn 2012, we launched a collaborative training program that allowed students from both universities to work together and implement what they learned in the classroom in clinical settings ranging from home-based care to rural clinics. In just one week, students provided care to more than 600 women and children.\\nWe are also working with the University of Alabama at Birmingham to facilitate the development of a curriculum for a PhD nursing program at Kamuzu College of Nursing. This will be one of the few PhD nursing programs available in in sub-Saharan Africa.\\n100X Development Foundation, Inc. is registered 503 (c)(3) nonprofit organization. Donations are deductable to the full extent allowable under IRS regulations.'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5769d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what to do with thing we are used to? shuffling?\n",
    "shuffled_dataset = dataset.shuffle(buffer_size=10_000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cc8d20ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 892,\n",
       " 'text': 'In this role, she oversees the day-to-day operations of the agency’s motoring services divisions (Vehicle Titles & Registration, Motor Vehicles, Motor Carrier, Enforcement, Consumer Relations and the Automobile Burglary & Theft Prevention Authority) to ensure they are constantly improving and identifying opportunities to become more efficient and effective in service delivery.\\nMellott came to the TxDMV from Alaska’s Division of Motor Vehicles where she most recently served as deputy executive director and acting executive director where she led a major initiative to modernize and improve the customer service experience. Previous positions at the Alaska DMV include oversight of all large field offices and leading the driver licensing program.\\nMellott serves on the American Association of Motor Vehicle Administrators Unconventional Vehicle Working Group and has worked collaboratively with representatives from across the country to develop best practices for states to follow when it comes to titling rebuilt, reconstructed and re-bodied vehicles.\\nShe grew up in a family that operated a trucking company and spent several years as a truck driver giving her first-hand experience of the importance and challenges of the motor carrier industry.\\nWhat brought you to YPP? I was looking for a job and I had got a text message from my friend and he asked me if I wanted to join YPP and I said yeah.\\nWhat do you enjoy most about YPP? What I enjoy the most is being able to mature and having more patience with the kids.\\nWhy do you think YPP is important? I think YPP is important because I feel like it will help educate young kids on their math skills and we’re giving back to the community.\\nHow has participating in YPP affected you? In YPP affected me by helping me improve on my growth about education and helped my character.'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(shuffled_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "48e2a8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n"
     ]
    }
   ],
   "source": [
    "print(dataset.n_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7e554d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset.set_epoch(epoch) # seed -> seed + epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb4f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
