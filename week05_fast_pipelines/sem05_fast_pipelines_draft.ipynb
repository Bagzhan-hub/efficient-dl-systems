{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f720956",
   "metadata": {},
   "source": [
    "# Week 5: Fast pipelines, profiling\n",
    "\n",
    "### Seminar outline\n",
    "- Automatic mixed precision and tensor cores\n",
    "- Efficient batching\n",
    "- Efficient pipelines with NVidia DALI\n",
    "- HuggingFace streaming dataset\n",
    "- Image decoders benchmarks\n",
    "- General purpose Python profiling with `py-spy`\n",
    "- Deep Learning profiling with `torch.utils.bottleneck()`\n",
    "- Profiling with `nvprof`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b7b98",
   "metadata": {},
   "source": [
    "## Automatic mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3fb3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 11 14:55:27 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:17.0 Off |                    0 |\n",
      "| N/A   50C    P0    70W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:18.0 Off |                    0 |\n",
      "| N/A   40C    P0    45W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:19.0 Off |                    0 |\n",
      "| N/A   38C    P0    45W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1A.0 Off |                    0 |\n",
      "| N/A   42C    P0    45W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   41C    P0    44W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   40C    P0    45W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   40C    P0    44W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   41C    P0    45W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a06f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace datasets\n",
    "# !pip install datasets\n",
    "\n",
    "# NVidia DALI\n",
    "# Make sure to choose correct CUDA version according to the nvidia-smi output\n",
    "# !pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0d9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from tqdm.auto import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1975d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    \"./mnist/\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ") \n",
    "mnist_val = torchvision.datasets.MNIST(\n",
    "    \"./mnist/\",\n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_train, batch_size=1024, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(mnist_val, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c410bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, n_epochs=3, device=\"cuda:0\", precision=\"full\"):\n",
    "    if precision == \"half\":\n",
    "        model.half()\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_train, y_train in tqdm(train_dataloader, desc=f\"Epoch {epoch}: \"):\n",
    "            if precision == \"half\":\n",
    "                x_train = x_train.half()\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "            y_pred = model(x_train)\n",
    "            loss = loss_fn(y_pred.float(), y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 2 == 0 or epoch == n_epochs - 1:\n",
    "            print(\"Starting validation...\")\n",
    "            model.eval()\n",
    "            val_loss = torch.empty(len(val_dataloader))\n",
    "            val_accuracy = torch.empty(len(val_dataloader))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for i, (x_val, y_val) in enumerate(val_dataloader):\n",
    "                    if precision == \"half\":\n",
    "                        x_val = x_val.half()\n",
    "                    x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                    y_pred = model(x_val)\n",
    "                    loss = loss_fn(y_pred.float(), y_val)\n",
    "                    val_loss[i] = loss\n",
    "                    val_accuracy[i] = (torch.argmax(y_pred, dim=-1) == y_val).float().mean()\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, loss: {val_loss.mean().detach().cpu()}, \"\n",
    "                f\"accuracy: {val_accuracy.mean().detach().cpu()}\"\n",
    "            )\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42d6e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175aa46abd634022bfe8e96cdbcc4afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation...\n",
      "Epoch: 0, loss: 0.10469746589660645, accuracy: 0.9667630195617676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ec4717891241a4859ba65502c8e96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877867efd1f24bd0aeaa580683f09004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation...\n",
      "Epoch: 2, loss: 0.06188463047146797, accuracy: 0.9779775738716125\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4),\n",
    "    nn.Conv2d(in_channels=20, out_channels=10, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2*2*10, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 10)\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model.to(\"cuda:0\"), loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4c4267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac4540fe4de4662b134c2bca9b9cdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation...\n",
      "Epoch: 0, loss: nan, accuracy: 0.09818439185619354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33a6d3a22f2464591e84d95d4b368ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6076360e06944cba0006f75080b4f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation...\n",
      "Epoch: 2, loss: nan, accuracy: 0.09818439185619354\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=4),\n",
    "    nn.Conv2d(in_channels=20, out_channels=10, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2*2*10, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 10)\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model.to(\"cuda:0\"), loss, optimizer, precision=\"half\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdac7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing utilities\n",
    "start_time = None\n",
    "def start_timer():\n",
    "    global start_time\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time()\n",
    "\n",
    "def end_timer_and_print(local_msg):\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time()\n",
    "    print(\"\\n\" + local_msg)\n",
    "    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n",
    "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aac0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(in_size, out_size, num_layers):\n",
    "    layers = []\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(torch.nn.Linear(in_size, in_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(in_size, out_size))\n",
    "    return torch.nn.Sequential(*tuple(layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb66edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512 # Try, for example, 128, 256, 513\n",
    "in_size = 4096 + 2048\n",
    "out_size = 4096 + 2048\n",
    "num_layers = 3\n",
    "num_batches = 50\n",
    "epochs = 3\n",
    "\n",
    "# Creates data in default precision.\n",
    "# The same data is used for both default and mixed precision trials below.\n",
    "# You don't need to manually change inputs' dtype when enabling mixed precision.\n",
    "data = [torch.randn(batch_size, in_size, device=\"cuda:0\") for _ in range(num_batches)]\n",
    "targets = [torch.randn(batch_size, out_size, device=\"cuda:0\") for _ in range(num_batches)]\n",
    "\n",
    "loss_fn = torch.nn.MSELoss().to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75cde7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/ml/lib/python3.9/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdce638717c44579548b09177b7febe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default precision:\n",
      "Total execution time = 4.182 sec\n",
      "Max memory used by tensors = 2379551744 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "net.to(\"cuda:0\")\n",
    "\n",
    "start_timer()\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        output = net(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "end_timer_and_print(\"Default precision:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "566120f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f4c8799c9549e985c022be41d16a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Half precision:\n",
      "Total execution time = 0.977 sec\n",
      "Max memory used by tensors = 2429897728 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "net.half()\n",
    "net.to(\"cuda:0\")\n",
    "\n",
    "start_timer()\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        output = net(input.half())\n",
    "        loss = loss_fn(output, target.half())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "end_timer_and_print(\"Half precision:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d3f703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf0334550f64354bf20ca85d3c73fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed precision without scaling:\n",
      "Total execution time = 1.493 sec\n",
      "Max memory used by tensors = 2499077120 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "net.to(\"cuda:0\")\n",
    "\n",
    "start_timer()\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            output = net(input)\n",
    "            # output is float16 because linear layers autocast to float16.\n",
    "            assert output.dtype is torch.float16\n",
    "            \n",
    "            loss = loss_fn(output, target)\n",
    "            # loss is float32 because mse_loss layers autocast to float32.\n",
    "            assert loss.dtype is torch.float32\n",
    "            \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "end_timer_and_print(\"Mixed precision without scaling:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08d1ccf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395389dc08c847b3be5c45bd3b6446f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed precision with autocast and scaling:\n",
      "Total execution time = 1.693 sec\n",
      "Max memory used by tensors = 2895539200 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "net.to(\"cuda:0\")\n",
    "\n",
    "start_timer()\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            output = net(input)\n",
    "            # output is float16 because linear layers autocast to float16.\n",
    "            assert output.dtype is torch.float16\n",
    "            \n",
    "            loss = loss_fn(output, target)\n",
    "            # loss is float32 because mse_loss layers autocast to float32.\n",
    "            assert loss.dtype is torch.float32\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "end_timer_and_print(\"Mixed precision with autocast and scaling:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611dbee",
   "metadata": {},
   "source": [
    "### Gradients modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac197fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0):\n",
    "    for input, target in zip(data, targets):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = net(input)\n",
    "            loss = loss_fn(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(opt)\n",
    "\n",
    "        # Since the gradients of optimizer's assigned params are now unscaled, clips as usual\n",
    "        # You may use the same value for max_norm here as you would without gradient scaling\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.1)\n",
    "\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d989524b",
   "metadata": {},
   "source": [
    "### Tensor cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "404a42e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f299ec60174aceaba565292a82a20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default precision, batch_size 512:\n",
      "Total execution time = 4.177 sec\n",
      "Max memory used by tensors = 5188050944 bytes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060b9a3cdc8a43b9a67c7f29e7a46f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default precision, batch_size 513:\n",
      "Total execution time = 4.576 sec\n",
      "Max memory used by tensors = 5188149248 bytes\n"
     ]
    }
   ],
   "source": [
    "data_512 = [torch.randn(512, in_size, device=\"cuda:0\") for _ in range(num_batches)]\n",
    "targets_512 = [torch.randn(512, out_size, device=\"cuda:0\") for _ in range(num_batches)]\n",
    "\n",
    "data_513 = [torch.randn(513, in_size, device=\"cuda:0\") for _ in range(num_batches)]\n",
    "targets_513 = [torch.randn(513, out_size, device=\"cuda:0\") for _ in range(num_batches)]\n",
    "\n",
    "loss_fn = torch.nn.MSELoss().to(\"cuda:0\")\n",
    "\n",
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "net.to(\"cuda:0\")\n",
    "\n",
    "start_timer()\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data_512, targets_512):\n",
    "        output = net(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "end_timer_and_print(\"Default precision, batch_size 512:\")\n",
    "\n",
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "net.to(\"cuda:0\")\n",
    "\n",
    "start_timer()\n",
    "for epoch in trange(epochs):\n",
    "    for input, target in zip(data_513, targets_513):\n",
    "        output = net(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "end_timer_and_print(\"Default precision, batch_size 513:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34235eea",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "Standard batching approach is just to stack tensors aquired with `__getitem__`.\n",
    "\n",
    "- Sequences with varying length\n",
    "- Different labels dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b4e4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca200720",
   "metadata": {},
   "source": [
    "brain: pad everything to a fixed max_length\n",
    "\n",
    "big brain: pad only in the collate_fn\n",
    "\n",
    "ultra duper big brain: presort data to sample sequences smartly, preserving +- the same length in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db0ada49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch:\n",
      "tensor([[ 2, 58, 44, 54, 21,  7, 75,  2, 19],\n",
      "        [67, 26,  2, 40,  3,  6, 17, 84,  0]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[ 1,  8, 48, 28,  0,  0,  0,  0,  0,  0],\n",
      "        [22,  9,  4, 15,  1, 14,  3,  6,  7, 18]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[22,  9,  4, 15,  1, 14,  3,  6,  7, 18],\n",
      "        [57, 73,  9, 65, 20,  2, 37, 74, 69,  0]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[31, 13, 64, 24, 13, 11, 83,  2, 19, 10, 16],\n",
      "        [39,  3,  6, 60, 25, 32,  0,  0,  0,  0,  0]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[ 4, 34, 76, 59, 30],\n",
      "        [ 5, 45,  9, 42, 11]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[ 1, 14,  3,  6,  7, 18, 53, 16, 27, 77],\n",
      "        [ 2, 19, 10, 16, 21, 50, 10, 47,  0,  0]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[11, 55, 13, 80, 26,  0,  0,  0],\n",
      "        [12,  4, 11, 23, 70, 25,  4, 86]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[ 2, 51,  5, 46,  5, 61,  1,  8],\n",
      "        [82,  1, 63,  5, 62, 85, 43,  0]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[78,  3,  5, 52, 66, 20, 38,  3,  6,  7, 17],\n",
      "        [ 1,  3, 71, 10, 79,  0,  0,  0,  0,  0,  0]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[12,  1, 49, 33, 29,  4, 68, 72,  0],\n",
      "        [ 2, 81,  1,  8, 56,  5, 12, 27, 23]])\n",
      "\n",
      "Current batch:\n",
      "tensor([[12,  1, 35, 41,  5,  4, 15, 24,  4, 36],\n",
      "        [ 8,  2, 17,  0,  0,  0,  0,  0,  0,  0]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: substitute with DataSet with __getitem__\n",
    "lines = [\n",
    "    \"One thing I don't know why\",\n",
    "    \"It doesn't even matter how hard you try\",\n",
    "    \"Keep that in mind, I designed this rhyme\",\n",
    "    \"To explain in due time\",\n",
    "    \"All I know\",\n",
    "    \"Time is a valuable thing\",\n",
    "    \"Watch it fly by as the pendulum swings\",\n",
    "    \"Watch it count down to the end of the day\",\n",
    "    \"The clock ticks life away\",\n",
    "    \"It's so unreal\",\n",
    "    \"Didn't look out below\",\n",
    "    \"Watch the time go right out the window\",\n",
    "    \"Tryin' to hold on, did-didn't even know\",\n",
    "    \"I wasted it all just to watch you go\",\n",
    "    \"I kept everything inside and even though I tried\",\n",
    "    \"It all fell apart\",\n",
    "    \"What it meant to me will eventually\",\n",
    "    \"Be a memory of a time when I tried so hard\",\n",
    "    \"I tried so hard and got so far\",\n",
    "    \"But in the end it doesn't even matter\",\n",
    "    \"I had to fall to lose it all\",\n",
    "    \"But in the end it doesn't even matter\"\n",
    "]\n",
    "labels = torch.randint(2, (len(lines), ))\n",
    "dataset = list(zip(lines, labels))\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, label in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(iter(dataset)), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "\n",
    "def collate_batch(batch: list[tuple[str, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    text_list, label_list = [], []\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "  \n",
    "    text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "  \n",
    "    return text_list, label_list\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=2, \n",
    "    collate_fn=collate_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for x, _ in dataloader:\n",
    "    print(f\"Current batch:\\n{x}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23bdfc9",
   "metadata": {},
   "source": [
    "Also check out `transformers.DataCollatorWithPadding` at https://huggingface.co/docs/transformers/main_classes/data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f615de33",
   "metadata": {},
   "source": [
    "## Data preprocessing with DALI\n",
    "https://docs.nvidia.com/deeplearning/dali/user-guide/docs/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd7be0",
   "metadata": {},
   "source": [
    "Augmentations: https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/image_processing/augmentation_gallery.html\n",
    "\n",
    "TODO: select simple example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46eb676",
   "metadata": {},
   "source": [
    "For images check out source for ResNet50 on ImageNet: https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/use_cases/pytorch/resnet50/pytorch-resnet50.ht"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836dae5",
   "metadata": {},
   "source": [
    "## Albumentations\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908b398",
   "metadata": {},
   "source": [
    "## Streaming datasets\n",
    "\n",
    "What if we do not want to wait until dataset is downloaded? Terrabytes of data.\n",
    "\n",
    "What if we do not have enough disk space?\n",
    "\n",
    "https://huggingface.co/docs/datasets/dataset_streaming.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d90a837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d57cd92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c82c5ca742640da895778d2b8ea2292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3b4aa6d7664c1a8c642a06fa604276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/359k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'text': 'Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help.\\nEstablished in honor of John & Lindy’s son, Christopher Blanchard, this particular program is very dear to the Blanchard family. Dana Blanchard, or Mama Dana as she is more commonly referred to at Mtendere, lived on site during the initial development, and she returns each summer to spend the season with her Malawian family. The heart of the program is to be His hands and feet by caring for the children at Mtendere, and meeting their spiritual, physical, academic, and emotional needs.\\nMtendere Village is home to 134 children, living in 16 homes with a housemother and several brothers and sisters. This family environment is one that many of the children have never previously experienced. 100X has also built a library and multipurpose building on the property—giving the children a place to gather together, learn and build community.\\nThe children attend a local primary school and receive afterschool tutoring at home in the afternoon. All of the school fees, uniforms, and supplies are paid for through our sponsorship program. Once children progress to secondary school (9th grade), they attend boarding school and return home during school breaks. Through education, these children will have the resources necessary to provide a better future for themselves and for their families—one where they are never faced with the decision of whether or not to abandon their own children.\\nMaternal Health. Right now, 1 in 18 women in Malawi will die during their lifetime due to complications and the lack of medical care during pregnancy and delivery. For every 100,000 births that occur in Malawi this year, more than 800 mothers will die during delivery or shortly after because of complications. The correlation between high rates of maternal mortality and the number of orphaned children in a country is strong, and Malawi estimates that there are already more than one million orphans.\\n100X Development is working with several key government, nonprofit, and academic partners to develop the Live to Love program. Live to Love will provide education, nutrition and HIV screening to pregnant mothers, and then strengthen the capacity of midwives and community healthcare workers to respond to the needs of mothers in areas where obstetric care is systematically lacking.\\nCommunity Health. Through local collaboration with partners such as Blessings Hospital, Lumbadzi Clinic and the Kamuzu College of Nursing, we provide health assessments and treatment to women and children in the rural villages of Malawi. For many, this is their only access to healthcare.\\nIn some respects, it was the partnership with Blessings Hospital that catalyzed our involvement in Malawi. We are committed to expanding resources available through Blessings, and are currently working with them to provide an outpatient clinic, a free “Under 5” clinic, a weekly pediatric physical therapy session, and a free ART clinic for patients who are HIV+.\\nNursing Education. In a country with only 16 obstetricians serving a population of over 16 million, it is evident that nurses play a critical role in the healthcare system. With high rates of maternal death, and the 10th highest infant mortality rate in the world, it is essential that the hospitals and clinics in Malawi be staffed with fully trained medical personnel. Unfortunately, while there are more nurses than doctors, the human resource shortage also extends to nurses, and services in rural areas are limited.\\nThrough partnership with Kamuzu College of Nursing (Malawi) and Auburn University’s School of Nursing (US), we are working to train and equip nursing students with the resources they need to care for the well-being of every patient.\\nIn 2012, we launched a collaborative training program that allowed students from both universities to work together and implement what they learned in the classroom in clinical settings ranging from home-based care to rural clinics. In just one week, students provided care to more than 600 women and children.\\nWe are also working with the University of Alabama at Birmingham to facilitate the development of a curriculum for a PhD nursing program at Kamuzu College of Nursing. This will be one of the few PhD nursing programs available in in sub-Saharan Africa.\\n100X Development Foundation, Inc. is registered 503 (c)(3) nonprofit organization. Donations are deductable to the full extent allowable under IRS regulations.'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5769d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what to do with thing we are used to? shuffling?\n",
    "shuffled_dataset = dataset.shuffle(buffer_size=10_000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc8d20ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 892,\n",
       " 'text': 'In this role, she oversees the day-to-day operations of the agency’s motoring services divisions (Vehicle Titles & Registration, Motor Vehicles, Motor Carrier, Enforcement, Consumer Relations and the Automobile Burglary & Theft Prevention Authority) to ensure they are constantly improving and identifying opportunities to become more efficient and effective in service delivery.\\nMellott came to the TxDMV from Alaska’s Division of Motor Vehicles where she most recently served as deputy executive director and acting executive director where she led a major initiative to modernize and improve the customer service experience. Previous positions at the Alaska DMV include oversight of all large field offices and leading the driver licensing program.\\nMellott serves on the American Association of Motor Vehicle Administrators Unconventional Vehicle Working Group and has worked collaboratively with representatives from across the country to develop best practices for states to follow when it comes to titling rebuilt, reconstructed and re-bodied vehicles.\\nShe grew up in a family that operated a trucking company and spent several years as a truck driver giving her first-hand experience of the importance and challenges of the motor carrier industry.\\nWhat brought you to YPP? I was looking for a job and I had got a text message from my friend and he asked me if I wanted to join YPP and I said yeah.\\nWhat do you enjoy most about YPP? What I enjoy the most is being able to mature and having more patience with the kids.\\nWhy do you think YPP is important? I think YPP is important because I feel like it will help educate young kids on their math skills and we’re giving back to the community.\\nHow has participating in YPP affected you? In YPP affected me by helping me improve on my growth about education and helped my character.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(shuffled_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48e2a8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n"
     ]
    }
   ],
   "source": [
    "print(dataset.n_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7e554d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset.set_epoch(epoch) # seed -> seed + epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb4f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
